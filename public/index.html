<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Devastate: Julia Galef's Scout Mindset-based trivia calibration" />
  <base href="%PUBLIC_URL%" />
  <link rel="stylesheet" type="text/css" href="index.css" />
  <title>Devastate</title>
</head>

<body>
  <p>So.</p>
  <p><a href="https://juliagalef.com/">Julia Galef</a> has this lovely book, <em><a
        href="https://www.penguinrandomhouse.com/books/555240/the-scout-mindset-by-julia-galef/">The Scout Mindset</a></em>, and there’s a delightful
    game in chapter six, “How sure are you?”, where you attempt to <em>calibrate your uncertainty</em> through a quick series of trivia questions.
    Just answer the questions below and then indicate your confidence. Per Galef:
  <blockquote>“As you go through the list, you should notice your level of certainty fluctuating. Some questions might feel easy, and you’ll be near
    certain of the answer. Others may prompt you to throw up your hands and say, ‘I have no idea!’ That’s perfectly fine. Remember, the goal isn’t to
    know as much as possible. It’s to know how much you know.”</blockquote>
  </p>
  <p>
    Someone with a good uncertainty calibration will miss around half of the questions they put in the “55%” uncertainty bucket while only missing one
    out of twenty questions they put in the “95%” bucket. The scatter plot between their <em>predicted</em> versus <em>actual</em> uncertainty will
    closely follow the <em>x</em>=<em>y</em> diagonal line below. When you’re done answering <em>all</em> questions, check the plot to measure your
    uncertainty calibration.
  </p>
  <div id="plot"></div>
  <div id="root"></div>
  <script type="module" src="dist/index.js"></script>
</body>

</html>